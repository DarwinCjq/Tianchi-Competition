import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import os
from PIL import Image
from sklearn.model_selection import train_test_split
import time
import cv2
start = time.time()
os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 使用第一个GPU

dir0 = 'F:/tianchiCOMPETITION/train_zhengchang_crop224/'
dir1 = 'F:/tianchiCOMPETITION/train_resize224feizhengchang1/'
#dir2 = 'D:/xuelangAI/train_resize64feizhengchang2/'
#dir3 = 'D:/xuelangAI/train_resize64feizhengchang3/'
dir4 = 'F:/tianchiCOMPETITION/test_crop224/'
#dir0 = 'D:/xuelangAI/train_resize64正常/'
#dir1 = 'D:/xuelangAI/train_resize64非正常/'
files0 = os.listdir(dir0)
files1 = os.listdir(dir1)
#files2 = os.listdir(dir2)
#files3 = os.listdir(dir3)
files4 = os.listdir(dir4)
train_data = []
test_data = []
label=[]

# 将图片转为数组
def load_data(imgName, num, path, a, b):
    for i in range(0,num):
        im = Image.open(path+imgName[i])
        arr = np.asarray(im,dtype='float32')
        train_data.append(arr)
        label.append([a,b])
        if i % 50 == 0:
            print('Converting training images No.%d image to array'%i)

# 将训练集的图片转为数组
load_data(files0, len(files0), dir0, 0.0, 1.0)
load_data(files1, len(files1), dir1, 1.0, 0.0)
#load_data(files2, len(files2), dir2, 1.0, 0.0)
#load_data(files3, len(files3), dir3, 1.0, 0.0)

# 将测试集的图片转为数组
for i in range(0, len(files4)):
    im = Image.open(dir4+files4[i])
    arr = np.asarray(im, dtype='float32')
    test_data.append(arr)
    if i % 50 == 0:
        print('Converting testing images No.%d image to array'%i)

# 把list 转为 数组
label = np.array(label)
train_data = np.array(train_data)
test_data = np.array(test_data)

# 归一化
def norm(size, dataset):
    for i in range(0, size):
        dataset[i] = (dataset[i] - np.min(dataset[i])) / (np.max(dataset[i] - np.min(dataset[i])))

norm(len(train_data), train_data)
norm(len(test_data), test_data)

print('************permutate training data...\n')
permutation = np.random.permutation(train_data.shape[0])
train_data = train_data[permutation, :,:]
label = label[permutation]


#X_train, X_test, y_train, y_test = train_test_split(train_data, label, test_size=0.25, random_state=0)
#X_train = np.array(X_train)
#X_test = np.array(X_test)
#y_train = np.array(y_train)
#y_test = np.array(y_test)

print('*****************start running CNN...\n')
# 训练的迭代次数
training_iters = 1
# learning rate: 降低 cost/loss/cross entropy
learning_rate = 0.0001
# batch size: 将训练图片分成一个固定的size,每个batch存储一定数量的图片
batch_size = 32 # power of 2

# MNIST data input (img shape: 28*28)
n_input = 64
# MNIST total classes (0-9 digits)
n_class = 2


def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.01)
    return tf.Variable(initial)

def get_weight(shape, lambda1):
    var = tf.Variable(tf.random_normal(shape, mean=0.0, stddev=0.01), dtype=tf.float32) # 生成一个变量
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lambda1)(var)) # add_to_collection()函数将新生成变量的L2正则化损失加入集合losses
    return var # 返回生成的变量

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

# define placeholder for inputs to network
xs = tf.placeholder(tf.float32, [None, 224,224,3])   # 32x32
ys = tf.placeholder(tf.float32, [None, n_class])
is_training = tf.placeholder(tf.bool)


def cnn_with_BN(x, is_training):
    
    #*******Convolution layer1
    #W_conv1 = weight_variable([3,3,3,32])
    W_conv1_1 = get_weight([3,3,3,32],0.003) # patch 3x3, in size 3, out size 64
    b_conv1_1 = bias_variable([32])
    W_conv1_2 = get_weight([3,3,32,32],0.003) # patch 3x3, in size 32, out size 64
    b_conv1_2 = bias_variable([32])
    
    h_conv1_1 = tf.layers.batch_normalization(conv2d(x, W_conv1_1)+b_conv1_1, training=is_training)
    h_conv1_1 = tf.nn.relu(h_conv1_1) # No.1 layer
    h_conv1_2 = tf.layers.batch_normalization(conv2d(h_conv1_1, W_conv1_2)+b_conv1_2, training=is_training)
    h_conv1_2 = tf.nn.relu(h_conv1_2) # No.2 layer
    h_pool1 = max_pool_2x2(h_conv1_2) #112*112*32                                

    #*******Convolution layer2
    #W_conv2 = weight_variable([3,3,32,64]) 
    W_conv2_1 = weight_variable([3,3,32,64]) # patch 3x3, in size 32, out size 64
    b_conv2_1 = bias_variable([64])
    W_conv2_2 = weight_variable([3,3,64,64]) # patch 3x3, in size 64, out size 64
    b_conv2_2 = bias_variable([64])
   
    h_conv2_1 = tf.layers.batch_normalization(conv2d(h_pool1, W_conv2_1)+b_conv2_1, training=is_training)
    h_conv2_1 = tf.nn.relu(h_conv2_1) # No.3 layer
    h_conv2_2 = tf.layers.batch_normalization(conv2d(h_conv2_1, W_conv2_2)+b_conv2_2, training=is_training)
    h_conv2_2 = tf.nn.relu(h_conv2_2) # No.4 layer
    h_pool2 = max_pool_2x2(h_conv2_2) #56*56*64                                        

    #*******Convolution layer3
    #W_conv3 = weight_variable([3,3,64,128]) # patch 3x3, in size 64, out size 128
    W_conv3_1 = weight_variable([3,3,64,128]) # patch 3x3, in size 64, out size 128
    b_conv3_1 = bias_variable([128])
    W_conv3_2 = weight_variable([3,3,128,128]) # patch 3x3, in size 128, out size 128
    b_conv3_2 = bias_variable([128])
    W_conv3_3 = weight_variable([3,3,128,128]) # patch 3x3, in size 128, out size 128
    b_conv3_3 = bias_variable([128])

    h_conv3_1 = tf.layers.batch_normalization(conv2d(h_pool2, W_conv3_1)+b_conv3_1, training=is_training)
    h_conv3_1 = tf.nn.relu(h_conv3_1) # No.5 layer
    h_conv3_2 = tf.layers.batch_normalization(conv2d(h_conv3_1, W_conv3_2)+b_conv3_2, training=is_training)
    h_conv3_2 = tf.nn.relu(h_conv3_2) # No.6 layer
    h_conv3_3 = tf.layers.batch_normalization(conv2d(h_conv3_2, W_conv3_3)+b_conv3_3, training=is_training)
    h_conv3_3 = tf.nn.relu(h_conv3_3) # No.7 layer
    h_pool3 = max_pool_2x2(h_conv3_3)   #28*28*128        
    
    #*******Convolution layer4
    W_conv4_1 = weight_variable([3,3,128,256]) # patch 3x3, in size 128, out size 256
    b_conv4_1 = bias_variable([256])
    W_conv4_2 = weight_variable([3,3,256,256]) # patch 3x3, in size 256, out size 256
    b_conv4_2 = bias_variable([256])
    W_conv4_3 = weight_variable([3,3,256,256]) # patch 3x3, in size 128, out size 128
    b_conv4_3 = bias_variable([256])

    h_conv4_1 = tf.layers.batch_normalization(conv2d(h_pool3, W_conv4_1)+b_conv4_1, training=is_training)
    h_conv4_1 = tf.nn.relu(h_conv4_1) # No.8 layer
    h_conv4_2 = tf.layers.batch_normalization(conv2d(h_conv4_1, W_conv4_2)+b_conv4_2, training=is_training)
    h_conv4_2 = tf.nn.relu(h_conv4_2) # No.9 layer
    h_conv4_3 = tf.layers.batch_normalization(conv2d(h_conv4_2, W_conv4_3)+b_conv4_3, training=is_training)
    h_conv4_3 = tf.nn.relu(h_conv4_3) # No.10 layer
    h_pool4 = max_pool_2x2(h_conv4_3)   #14*14*256    

    #*******Convolution layer4
    W_conv5_1 = weight_variable([3,3,256,512]) # patch 3x3, in size 256, out size 512
    b_conv5_1 = bias_variable([512])
    W_conv5_2 = weight_variable([3,3,512,512]) # patch 3x3, in size 512, out size 512
    b_conv5_2 = bias_variable([512])
    W_conv5_3 = weight_variable([3,3,512,512]) # patch 3x3, in size 512, out size 512
    b_conv5_3 = bias_variable([512])

    h_conv5_1 = tf.layers.batch_normalization(conv2d(h_pool4, W_conv5_1)+b_conv5_1, training=is_training)
    h_conv5_1 = tf.nn.relu(h_conv5_1) # No.11 layer
    h_conv5_2 = tf.layers.batch_normalization(conv2d(h_conv5_1, W_conv5_2)+b_conv5_2, training=is_training)
    h_conv5_2 = tf.nn.relu(h_conv5_2) # No 12 layer
    h_conv5_3 = tf.layers.batch_normalization(conv2d(h_conv5_2, W_conv5_3)+b_conv5_3, training=is_training)
    h_conv5_3 = tf.nn.relu(h_conv5_3) # No.13 layer
    h_pool5 = max_pool_2x2(h_conv5_3)   #7*7*512   

    W_fc1 = get_weight([7*7*512,128],0.003)
    b_fc1 = bias_variable([128])
    h_pool5_flat = tf.reshape(h_pool5, [-1, 7*7*512])
    h_pool5_flat = tf.layers.batch_normalization(tf.matmul(h_pool5_flat, W_fc1)+b_fc1, training=is_training)
    h_fc1 = tf.nn.relu(h_pool5_flat)  # No.14 layer

    W_fc2 = get_weight([128, 64],0.003)
    b_fc2 = bias_variable([64])
    h_fc2 = tf.layers.batch_normalization(tf.matmul(h_fc1, W_fc2)+b_fc2, training=is_training)
    h_fc2 = tf.nn.relu(h_fc2)         # No.15 layer

    W_fc3 = get_weight([64, n_class],0.003)
    b_fc3 = bias_variable([n_class])
    pred = tf.matmul(h_fc2, W_fc3) + b_fc3 # No.16 layer
    return pred

out = cnn_with_BN(xs, is_training)
prediction = tf.nn.softmax(out)

cross_entropy = -tf.reduce_mean(ys*tf.log(tf.clip_by_value(prediction,1e-15,1.0)))
tf.add_to_collection('losses', cross_entropy)
# get_collection()返回一个列表，这个列表是所有这个集合中的元素，在本样例中这些元素就是损失函数的不同部分，将他们加起来就是最终的损失函数
cost = tf.add_n(tf.get_collection('losses'))
with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)

#correct = tf.equal(tf.argmax(out, 1), tf.argmax(ys, 1))
correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(ys, 1))
accuracy = tf.reduce_mean(tf.cast(correct, 'float'))


sess = tf.Session()
sess.run(tf.global_variables_initializer())

for i in range(training_iters):
    for batch in range(len(train_data)//batch_size+1):
        batch_x = train_data[batch*batch_size:min((batch+1)*batch_size,len(train_data))]
        batch_y = label[batch*batch_size:min((batch+1)*batch_size,len(label))]    
        opt = sess.run(optimizer, feed_dict={xs: batch_x, ys: batch_y, is_training:True})
        loss, acc = sess.run([cost, accuracy], feed_dict={xs: batch_x, ys: batch_y, is_training:False})

    #pb1 = tf.nn.softmax(cnn_with_BN(xs, is_training))
    pb = sess.run(prediction, feed_dict={xs:test_data[0:32], is_training:False})
    print('the probability of first 32 pics is:\n', pb)
    time_temp = time.time()
    print('running time: %s mins...\n'%((time_temp-start)/60))
    print("Iter " + str(i) + ", Loss= " + \
                      "{:.6f}".format(loss) + ", Training Accuracy= " + \
                      "{:.5f}".format(acc) + '\n')

print('*******************train is over,put test datasets...\n')
prob_list = []
for batch_test in range(len(test_data)//batch_size+1):
    batch = test_data[batch_test*batch_size:min((batch_test+1)*batch_size, len(test_data))]
    prob = sess.run(prediction, feed_dict={xs:batch, is_training:False})
    prob_list.append(prob)
print('******************the length of test datasets is: %d\n'%(len(prob_list)))

# 提取图片的概率
temp = []
#for i in range(0, len(prob_list)-1):
#    for j in range(0,batch_size):
      
#        tt = prob_list[i][j]
#        #print('**************Get No. %d batch，the softmax probability of No.%d picture is:'%(i,j))
#        temp.append(tt[0])
for i in range(0, len(prob_list)):
    for j in range(0,batch_size):
      
        tt = prob_list[i][j]
        #print('**************Get No. %d batch，the softmax probability of No.%d picture is:'%(i,j))
        temp.append(tt[0])

#for m in range(0, 32):
#    tt = prob_list[len(prob_list)-1][m]
#    temp.append(tt[0])
#print('***************the length of all probabilities:%d\n'%len(temp))
# 提取概率最大的
m = 0
prob_temp = []
for j in range(0, int(len(temp)/16)):
    t = temp[0+m:m+16]
    prob_temp.append(max(t))
    print('**************the maximum probability of No.%d picture is: %s'%(j,max(t)))
    m = m + 16

# 提取测试集的文件名
names = []
names_temp = []
for i in range(0, len(files4), 16):
    names_temp.append(files4[i])
# 删除文件名中的多余字符
for i in range(0, len(names_temp)):
    names.append(names_temp[i].replace('-0',''))

# 保存概率和文件名
print('\n*********************************save probabilities and filenames...\n')
import pickle
result1 = open('F:/tianchiCOMPETITION/prob.pickle', 'wb')
pickle.dump(prob_temp, result1)
result1.close()

result2 = open('F:/tianchiCOMPETITION/name.pickle', 'wb')
pickle.dump(names, result2)
result2.close()

print('*******************upload pickle files...\n')
print('the length of the filename:%d, the length of the probability: %d'%(len(names), len(prob_temp)))
import pandas as pd
dataframe = pd.DataFrame({'filename':names,'probability':prob_temp})
dataframe.to_csv("F:/tianchiCOMPETITION/test.csv",index=False,sep=',')

end = time.time()

print ('***************completed,the total running time is %s minutes\n'%((end-start)/60))





